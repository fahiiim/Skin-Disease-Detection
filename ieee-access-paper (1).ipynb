{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8972228,"sourceType":"datasetVersion","datasetId":5302785}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IEEE Access Paper: A Deep Learning Approach Based on Explainable Artificial Intelligence for Skin Lesion Classification\n**By: University of Engineering and Technology,Lahore,pakistan**","metadata":{}},{"cell_type":"markdown","source":"# Data Collection","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ndataset_path = \"/kaggle/input/all-isic-data-20240629\"\nmetadata = pd.read_csv(os.path.join(dataset_path, \"/kaggle/input/all-isic-data-20240629/metadata.csv\"), low_memory=False)\nimage_dir = os.path.join(dataset_path, \"images\")\n\n# Filter out NaN in diagnosis\nmetadata = metadata.dropna(subset=['diagnosis'])\nprint(f\"Filtered metadata size: {len(metadata)}\")\nprint(\"Unique diagnosis values:\", metadata['diagnosis'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:49:07.501927Z","iopub.execute_input":"2025-04-18T12:49:07.502592Z","iopub.status.idle":"2025-04-18T12:49:08.322724Z","shell.execute_reply.started":"2025-04-18T12:49:07.502565Z","shell.execute_reply":"2025-04-18T12:49:08.321966Z"}},"outputs":[{"name":"stdout","text":"Filtered metadata size: 53826\nUnique diagnosis values: ['nevus' 'melanoma' 'atypical melanocytic proliferation' 'scar'\n 'solar lentigo' 'seborrheic keratosis' 'actinic keratosis'\n 'basal cell carcinoma' 'squamous cell carcinoma' 'dermatofibroma'\n 'vascular lesion' 'lichenoid keratosis' 'lentigo NOS' 'verruca'\n 'clear cell acanthoma' 'angiofibroma or fibrous papule' 'angioma'\n 'atypical spitz tumor' 'AIMP' 'neurofibroma' 'lentigo simplex'\n 'acrochordon' 'angiokeratoma' 'other' 'cafe-au-lait macule'\n 'pigmented benign keratosis' 'melanoma metastasis' 'pyogenic granuloma'\n 'sebaceous adenoma' 'sebaceous hyperplasia' 'nevus spilus'\n 'mucosal melanosis']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Inspect diagnosis column\nprint(\"Diagnosis column values (first 10):\")\nprint(metadata['diagnosis'].head(10))\nprint(\"\\nUnique values in diagnosis:\")\nprint(metadata['diagnosis'].unique())\nprint(\"\\nValue counts:\")\nprint(metadata['diagnosis'].value_counts(dropna=False))\nprint(\"\\nData type:\")\nprint(metadata['diagnosis'].dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:49:08.323953Z","iopub.execute_input":"2025-04-18T12:49:08.324172Z","iopub.status.idle":"2025-04-18T12:49:08.335260Z","shell.execute_reply.started":"2025-04-18T12:49:08.324155Z","shell.execute_reply":"2025-04-18T12:49:08.334446Z"}},"outputs":[{"name":"stdout","text":"Diagnosis column values (first 10):\n0        nevus\n2        nevus\n5        nevus\n8        nevus\n9        nevus\n10       nevus\n13    melanoma\n15       nevus\n16       nevus\n17       nevus\nName: diagnosis, dtype: object\n\nUnique values in diagnosis:\n['nevus' 'melanoma' 'atypical melanocytic proliferation' 'scar'\n 'solar lentigo' 'seborrheic keratosis' 'actinic keratosis'\n 'basal cell carcinoma' 'squamous cell carcinoma' 'dermatofibroma'\n 'vascular lesion' 'lichenoid keratosis' 'lentigo NOS' 'verruca'\n 'clear cell acanthoma' 'angiofibroma or fibrous papule' 'angioma'\n 'atypical spitz tumor' 'AIMP' 'neurofibroma' 'lentigo simplex'\n 'acrochordon' 'angiokeratoma' 'other' 'cafe-au-lait macule'\n 'pigmented benign keratosis' 'melanoma metastasis' 'pyogenic granuloma'\n 'sebaceous adenoma' 'sebaceous hyperplasia' 'nevus spilus'\n 'mucosal melanosis']\n\nValue counts:\ndiagnosis\nnevus                                 32697\nmelanoma                               7349\nbasal cell carcinoma                   4921\nseborrheic keratosis                   1926\nsquamous cell carcinoma                1372\nactinic keratosis                      1367\npigmented benign keratosis             1339\nsolar lentigo                           562\ndermatofibroma                          420\nvascular lesion                         348\nlichenoid keratosis                     312\nacrochordon                             301\nlentigo NOS                             241\natypical melanocytic proliferation      143\nAIMP                                    121\nverruca                                 119\nangioma                                  71\nlentigo simplex                          43\nmelanoma metastasis                      39\nother                                    33\nscar                                     31\nneurofibroma                             26\nangiokeratoma                            13\nsebaceous hyperplasia                     7\nclear cell acanthoma                      6\natypical spitz tumor                      5\nangiofibroma or fibrous papule            4\npyogenic granuloma                        3\ncafe-au-lait macule                       2\nsebaceous adenoma                         2\nmucosal melanosis                         2\nnevus spilus                              1\nName: count, dtype: int64\n\nData type:\nobject\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Step-1: Image Preprocessing    \nTo enhance image quality and focus on the lesion areas, the following preprocessing steps are applied:​\n\n1.  **Objective:** Enhance image quality, standardize dimensions, and focus on regions of interest (ROIs).\n\n    I’ll preprocess images by:\n    \n    * Applying noise reduction (Gaussian blur).\n    * Resizing to 224x224 (standard for models like ResNet).\n    * Normalizing pixel values to [0, 1].\n    * Cropping ROIs (simplified to central cropping for now).\n1. **Implementation:**\n\nSince the dataset is large, we’ll preprocess images on-the-fly during training to save memory, using a generator.\nWe’ll assume metadata.csv has columns like isic_id (image filename without extension) and diagnosis (label).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom sklearn.preprocessing import LabelEncoder\n\nclass ISICDataset(torch.utils.data.Dataset):\n    def __init__(self, metadata, image_dir, transform=None):\n        self.metadata = metadata.reset_index(drop=True)  # Reset index after filtering\n        self.image_dir = image_dir\n        self.transform = transform\n        self.label_encoder = LabelEncoder()\n        self.labels = self.label_encoder.fit_transform(metadata['diagnosis'])\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        img_id = self.metadata.iloc[idx]['isic_id']  # Adjust if different\n        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except:\n            return None  # Skip invalid images\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Preprocessing transforms\npreprocess_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),  # Ensure images are large enough\n    transforms.CenterCrop(224),  # Crop to 224x224\n    transforms.GaussianBlur(kernel_size=5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:49:08.336073Z","iopub.execute_input":"2025-04-18T12:49:08.336360Z","iopub.status.idle":"2025-04-18T12:49:08.424870Z","shell.execute_reply.started":"2025-04-18T12:49:08.336335Z","shell.execute_reply":"2025-04-18T12:49:08.424048Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Step-2: Data Augmentation\nTo increase data diversity and prevent overfitting, the following augmentation techniques are applied:​\n\n* **Rotation:** Random rotations at various angles.​\n\n* **Flipping:** Horizontal and vertical flips.​\n\n* **Cropping:** Random crops to simulate zoom.​\n\n* **Brightness and Contrast Adjustment:** Randomly altering brightness and contrast levels.​\n\n* **Noise Addition:** Introducing random noise to images.","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\n# Training transforms with augmentation\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),  # Ensure images are large enough\n    transforms.RandomCrop(224),  # Now safe\n    transforms.RandomRotation(40),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.GaussianBlur(kernel_size=5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Validation transforms\nval_transforms = preprocess_transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:49:08.426585Z","iopub.execute_input":"2025-04-18T12:49:08.426936Z","iopub.status.idle":"2025-04-18T12:49:08.440200Z","shell.execute_reply.started":"2025-04-18T12:49:08.426917Z","shell.execute_reply":"2025-04-18T12:49:08.439575Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Step-3: Model Training with ResNet-18\nUtilizing transfer learning, a pre-trained ResNet-18 model is fine-tuned for skin lesion classification. The final fully connected layer is modified to output predictions for the nine classes present in the ISIC 2019 dataset.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Create datasets\ntrain_dataset = ISICDataset(metadata, image_dir, transform=train_transforms)\nval_dataset = ISICDataset(metadata, image_dir, transform=val_transforms)\n\n# Split indices\nindices = np.arange(len(metadata))\ntrain_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n\n# Define model\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nnum_classes = len(np.unique(metadata['diagnosis']))\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n# Move to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 50\nbest_val_acc = 0.0\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for data in train_loader:\n        if data is None:\n            continue\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n\n    # Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            if data is None:\n                continue\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n    print(f\"Validation Acc: {val_acc:.2f}%\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pth\")\n    if val_acc > 95:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:49:08.440973Z","iopub.execute_input":"2025-04-18T12:49:08.441235Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 196MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 1.1809, Train Acc: 65.09%\nValidation Acc: 67.71%\nEpoch 2, Loss: 1.0468, Train Acc: 67.88%\nValidation Acc: 69.07%\nEpoch 3, Loss: 1.0009, Train Acc: 68.70%\nValidation Acc: 69.30%\nEpoch 4, Loss: 0.9675, Train Acc: 69.29%\nValidation Acc: 70.42%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Model Explainability with the LIME\nTo interpret the model's predictions, LIME (Local Interpretable Model-Agnostic Explanations) is employed. LIME provides visual explanations by highlighting regions in the image that most influenced the model's decision.","metadata":{}},{"cell_type":"code","source":"from lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\n\ndef explain_prediction(model, image_tensor):\n    model.eval()\n    image = image_tensor.numpy().transpose(1, 2, 0)\n\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(\n        image, \n        classifier_fn=lambda x: model(torch.tensor(x).permute(0, 3, 1, 2).float()).detach().numpy(),\n        top_labels=1,\n        hide_color=0,\n        num_samples=1000\n    )\n\n    temp, mask = explanation.get_image_and_mask(\n        explanation.top_labels[0],\n        positive_only=True,\n        num_features=5,\n        hide_rest=False\n    )\n\n    plt.imshow(mark_boundaries(temp / 255.0, mask))\n    plt.title('LIME Explanation')\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSkin Lesion Classification with Explainable AI\nBased on \"A Deep Learning Approach Based on Explainable Artificial Intelligence for Skin Lesion Classification\"\n\nThis implementation combines the best elements from the IEEE Access paper notebook\nwith a complete training, evaluation, and explainability pipeline.\n\"\"\"\n\n# ===== Step 1: Import Required Libraries =====\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_filter\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, models\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Scikit-learn imports for evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n\n# LIME for explainability\nimport lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\n\n# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed()\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ===== Step 2: Define Constants =====\nIMG_SIZE = 224\nBATCH_SIZE = 32\nNUM_EPOCHS = 50\nLEARNING_RATE = 0.001\nNUM_CLASSES = 8  # ISIC 2019 has 8 classes as mentioned in the paper\n\n# Class names for ISIC 2019 dataset\nCLASS_NAMES = [\n    'Melanoma', \n    'Nevus',\n    'Basal Cell Carcinoma',\n    'Actinic Keratosis',\n    'Benign Keratosis',\n    'Dermatofibroma',\n    'Vascular Lesion',\n    'Squamous Cell Carcinoma'\n]\n\n# Define paths - update these according to your directory structure\nBASE_PATH = \"/kaggle/input/all-isic-data-20240629\"\nIMAGES_PATH = os.path.join(BASE_PATH, \"/kaggle/input/all-isic-data-20240629/images\")\nMETADATA_PATH = os.path.join(BASE_PATH, \"/kaggle/input/all-isic-data-20240629/metadata.csv\")\nGROUND_TRUTH_PATH = os.path.join(BASE_PATH, \"/kaggle/input/all-isic-data-20240629/metadata.csv\")\n\n# ===== Step 3: Image Preprocessing Functions =====\ndef preprocess_image(image_path):\n    \"\"\"\n    Preprocess an image according to the IEEE Access paper methodology:\n    - Center crop (ROI extraction)\n    - Resize to 224x224\n    - Zero padding\n    - Gaussian noise reduction\n    - Normalization\n    \"\"\"\n    # Load image\n    img = Image.open(image_path).convert('RGB')\n    img_np = np.array(img)\n\n    # Step 1: Center Crop (ROI Extraction)\n    h, w = img_np.shape[:2]\n    side = min(h, w)\n    startx = w//2 - side//2\n    starty = h//2 - side//2\n    cropped = img_np[starty:starty+side, startx:startx+side]\n\n    # Step 2: Resize to 224x224\n    resized = cv2.resize(cropped, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n\n    # Step 3: Zero Padding (if needed)\n    top, bottom, left, right = (0, 0, 0, 0)\n    max_side = max(resized.shape[:2])\n    delta_w = max_side - resized.shape[1]\n    delta_h = max_side - resized.shape[0]\n    if delta_w > 0 or delta_h > 0:\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n        padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n    else:\n        padded = resized\n\n    # Step 4: Gaussian Noise Reduction\n    denoised = gaussian_filter(padded, sigma=1)\n\n    # Step 5: Normalize to [0,1]\n    normalized = denoised / 255.0\n    \n    return normalized\n\n# ===== Step 4: Dataset and DataLoader Classes =====\nclass SkinLesionDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None, preprocess=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.preprocess = preprocess\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        \n        # Apply preprocessing as per IEEE paper\n        if self.preprocess:\n            image = preprocess_image(img_path)\n            image = torch.from_numpy(image).float().permute(2, 0, 1)  # Convert to tensor and rearrange to [C, H, W]\n        else:\n            # Just load the image\n            image = Image.open(img_path).convert('RGB')\n        \n        # Apply additional transformations (data augmentation)\n        if self.transform:\n            if self.preprocess:\n                # If already preprocessed and tensor, handle differently\n                image = self.transform(image)\n            else:\n                # If PIL Image\n                image = self.transform(image)\n            \n        label = self.labels[idx]\n        return image, label\n\ndef prepare_data():\n    \"\"\"\n    Load the ISIC 2019 dataset, preprocess it, and create DataLoaders\n    \"\"\"\n    print(\"Loading and preprocessing data...\")\n    \n    # Load metadata and ground truth\n    metadata = pd.read_csv(METADATA_PATH)\n    ground_truth = pd.read_csv(GROUND_TRUTH_PATH)\n    \n    # Merge the dataframes\n    full_data = pd.concat([metadata, ground_truth], axis=1)\n    \n    # Create image paths list and labels\n    image_paths = [os.path.join(IMAGES_PATH, f\"{img_id}.jpg\") for img_id in full_data['image']]\n    \n    # Convert one-hot encoded labels to class indices\n    class_columns = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n    labels = np.argmax(full_data[class_columns].values, axis=1)\n    \n    # Split data into train+val and test sets\n    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n        image_paths, labels, test_size=0.15, stratify=labels, random_state=42\n    )\n    \n    # Further split train+val into train and val sets\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        train_val_paths, train_val_labels, test_size=0.15, stratify=train_val_labels, random_state=42\n    )\n    \n    # Print dataset statistics\n    print(f\"Dataset split: Train={len(train_paths)}, Val={len(val_paths)}, Test={len(test_paths)}\")\n    \n    # Define data augmentation for training\n    train_transform = transforms.Compose([\n        transforms.RandomRotation(30),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # No augmentation for validation and test sets\n    val_test_transform = transforms.Compose([\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = SkinLesionDataset(train_paths, train_labels, transform=train_transform, preprocess=True)\n    val_dataset = SkinLesionDataset(val_paths, val_labels, transform=val_test_transform, preprocess=True)\n    test_dataset = SkinLesionDataset(test_paths, test_labels, transform=val_test_transform, preprocess=True)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    \n    return train_loader, val_loader, test_loader, test_paths, test_labels\n\n# ===== Step 5: Model Definition =====\ndef create_model():\n    \"\"\"\n    Create a ResNet-18 model with transfer learning, as specified in the paper\n    \"\"\"\n    print(\"Creating ResNet-18 model with transfer learning...\")\n    \n    # Load pretrained ResNet-18 model\n    model = models.resnet18(pretrained=True)\n    \n    # Freeze early layers but allow later layers to train\n    # This is a modification from the notebook where all layers were frozen\n    for name, param in model.named_parameters():\n        if 'layer4' not in name and 'fc' not in name:  # Only train the last convolutional block and FC layer\n            param.requires_grad = False\n    \n    # Modify the final fully connected layer for our classification task\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Sequential(\n        nn.Dropout(0.5),  # Add dropout for regularization\n        nn.Linear(num_ftrs, NUM_CLASSES)\n    )\n    \n    model = model.to(device)\n    \n    return model\n\n# ===== Step 6: Training Functions =====\ndef train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n    \"\"\"\n    Train the model and validate it on the validation set\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n    \n    # For tracking the best model\n    best_val_acc = 0.0\n    best_model_wts = None\n    \n    # Lists to store metrics for plotting\n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    print(\"Beginning training...\")\n    \n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n        # Calculate epoch training metrics\n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = correct / total\n        train_losses.append(epoch_train_loss)\n        train_accs.append(epoch_train_acc)\n        \n        print(f\"Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {epoch_train_acc:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader):\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        # Calculate epoch validation metrics\n        epoch_val_loss = running_loss / len(val_loader.dataset)\n        epoch_val_acc = correct / total\n        val_losses.append(epoch_val_loss)\n        val_accs.append(epoch_val_acc)\n        \n        print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}\")\n        \n        # Update learning rate scheduler\n        scheduler.step(epoch_val_acc)\n        \n        # Save the best model\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            best_model_wts = model.state_dict().copy()\n            print(f\"New best model saved with accuracy: {best_val_acc:.4f}\")\n            \n            # Save the model checkpoint\n            torch.save(model.state_dict(), 'best_model.pth')\n        \n        print(\"-\" * 50)\n    \n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    # Plot training and validation metrics\n    plot_training_metrics(train_losses, val_losses, train_accs, val_accs)\n    \n    return model\n\ndef plot_training_metrics(train_losses, val_losses, train_accs, val_accs):\n    \"\"\"\n    Plot training and validation metrics\n    \"\"\"\n    plt.figure(figsize=(12, 5))\n    \n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label='Train Accuracy')\n    plt.plot(val_accs, label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.close()\n\n# ===== Step 7: Model Evaluation =====\ndef evaluate_model(model, test_loader):\n    \"\"\"\n    Evaluate the model on the test set and print detailed metrics\n    \"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    print(\"Evaluating model on test set...\")\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Print metrics\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    \n    # Print detailed classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n    \n    # Plot confusion matrix\n    plot_confusion_matrix(cm, CLASS_NAMES)\n    \n    return accuracy, precision, recall, f1\n\ndef plot_confusion_matrix(cm, class_names):\n    \"\"\"\n    Plot the confusion matrix for test results\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    \n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45, ha='right')\n    plt.yticks(tick_marks, class_names)\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], 'd'),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png')\n    plt.close()\n\n# ===== Step 8: Explainable AI with LIME =====\ndef explain_predictions(model, test_paths, test_labels, num_samples=5):\n    \"\"\"\n    Use LIME to explain model predictions for sample images\n    \"\"\"\n    print(\"Generating LIME explanations for sample predictions...\")\n    \n    # Prepare the LIME explainer\n    explainer = lime_image.LimeImageExplainer()\n    \n    # Function to get preprocessed image as numpy array\n    def get_preprocessed_image(img_path):\n        preprocessed = preprocess_image(img_path)\n        return preprocessed\n    \n    # Function for model to make predictions on batch\n    def batch_predict(images):\n        \"\"\"\n        Function that takes a batch of images and returns the predictions\n        \"\"\"\n        model.eval()\n        batch = torch.stack([torch.from_numpy(img).float().permute(2, 0, 1) for img in images])\n        batch = batch.to(device)\n        \n        # Apply normalization\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        batch = torch.stack([normalize(img) for img in batch])\n        \n        with torch.no_grad():\n            outputs = model(batch)\n            probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n        return probs\n    \n    # Select random samples to explain\n    num_test_samples = len(test_paths)\n    indices = np.random.choice(range(num_test_samples), num_samples, replace=False)\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n    \n    for i, idx in enumerate(indices):\n        # Load image\n        img_path = test_paths[idx]\n        true_label = test_labels[idx]\n        true_class = CLASS_NAMES[true_label]\n        \n        # Get original image\n        orig_img = np.array(Image.open(img_path).convert('RGB'))\n        \n        # Get preprocessed image for model\n        preprocessed_img = get_preprocessed_image(img_path)\n        \n        # Get model prediction\n        tensor_img = torch.from_numpy(preprocessed_img).float().permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n        tensor_img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor_img)\n        tensor_img = tensor_img.to(device)\n        \n        with torch.no_grad():\n            output = model(tensor_img)\n            probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n            pred_idx = torch.argmax(probabilities).item()\n            pred_class = CLASS_NAMES[pred_idx]\n            pred_prob = probabilities[pred_idx].item()\n        \n        # Get LIME explanation\n        explanation = explainer.explain_instance(\n            preprocessed_img,\n            batch_predict,\n            top_labels=1,\n            hide_color=0,\n            num_samples=100\n        )\n        \n        # Get the explanation for the top predicted class\n        temp, mask = explanation.get_image_and_mask(\n            explanation.top_labels[0],\n            positive_only=True,\n            num_features=5,\n            hide_rest=False\n        )\n        \n        # Create visualization\n        if num_samples == 1:\n            # Handle the case with only one sample differently\n            axes[0].imshow(orig_img)\n            axes[0].set_title(f\"Original Image\\nTrue: {true_class}\")\n            axes[0].axis('off')\n            \n            axes[1].imshow(preprocessed_img)\n            axes[1].set_title(f\"Model Prediction\\n{pred_class} ({pred_prob:.2f})\")\n            axes[1].axis('off')\n            \n            axes[2].imshow(mark_boundaries(temp, mask))\n            axes[2].set_title(\"LIME Explanation\\n(Highlighted areas influenced the decision)\")\n            axes[2].axis('off')\n        else:\n            axes[i, 0].imshow(orig_img)\n            axes[i, 0].set_title(f\"Original Image\\nTrue: {true_class}\")\n            axes[i, 0].axis('off')\n            \n            axes[i, 1].imshow(preprocessed_img)\n            axes[i, 1].set_title(f\"Model Prediction\\n{pred_class} ({pred_prob:.2f})\")\n            axes[i, 1].axis('off')\n            \n            axes[i, 2].imshow(mark_boundaries(temp, mask))\n            axes[i, 2].set_title(\"LIME Explanation\\n(Highlighted areas influenced the decision)\")\n            axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('lime_explanations.png')\n    plt.close()\n    \n    print(f\"LIME explanations generated for {num_samples} sample images.\")\n\n# ===== Step 9: Main Function =====\ndef main():\n    \"\"\"\n    Main function to run the full process\n    \"\"\"\n    print(\"Starting the skin lesion classification process...\")\n    \n    # Step 1: Prepare data\n    train_loader, val_loader, test_loader, test_paths, test_labels = prepare_data()\n    \n    # Step 2: Create model\n    model = create_model()\n    \n    # Step 3: Check if a trained model exists, otherwise train the model\n    if os.path.exists('best_model.pth'):\n        print(\"Loading pre-trained model...\")\n        model.load_state_dict(torch.load('best_model.pth', map_location=device))\n    else:\n        print(\"Training model from scratch...\")\n        model = train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)\n    \n    # Step 4: Evaluate model\n    accuracy, precision, recall, f1 = evaluate_model(model, test_loader)\n    \n    # Step 5: Generate LIME explanations\n    explain_predictions(model, test_paths, test_labels, num_samples=5)\n    \n    print(\"Process completed successfully!\")\n    print(f\"Final metrics - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n          f\"Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}