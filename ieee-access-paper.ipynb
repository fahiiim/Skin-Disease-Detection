{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8972228,"sourceType":"datasetVersion","datasetId":5302785}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IEEE Access Paper: A Deep Learning Approach Based on Explainable Artificial Intelligence for Skin Lesion Classification\n**By: University of Engineering and Technology,Lahore,pakistan**","metadata":{}},{"cell_type":"markdown","source":"# Step-1: Image Preprocessing    \nTo enhance image quality and focus on the lesion areas, the following preprocessing steps are applied:​\n\n* **Region of Interest (ROI) Extraction:** Center cropping to isolate the lesion.​\n\n* **Resizing:** Adjusting images to 224×224 pixels.​\nchallenge.isic-archive.com\n+1\nKaggle\n+1\n\n* **Zero Padding:** Maintaining aspect ratio without distortion.​\n\n* **Noise Reduction:** Applying Gaussian filters to remove artifacts.​\n\n* **Normalization:** Scaling pixel values to the [0,1] range.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_filter\nimport matplotlib.pyplot as plt\n\ndef preprocess_image(image_path):\n    # Load image\n    img = Image.open(image_path).convert('RGB')\n    img_np = np.array(img)\n\n    # Step 1: Center Crop (ROI Extraction)\n    h, w = img_np.shape[:2]\n    side = min(h, w)\n    startx = w//2 - side//2\n    starty = h//2 - side//2\n    cropped = img_np[starty:starty+side, startx:startx+side]\n\n    # Step 2: Resize to 224x224\n    resized = cv2.resize(cropped, (224, 224), interpolation=cv2.INTER_AREA)\n\n    # Step 3: Zero Padding (if needed)\n    top, bottom, left, right = (0, 0, 0, 0)\n    max_side = max(resized.shape[:2])\n    delta_w = max_side - resized.shape[1]\n    delta_h = max_side - resized.shape[0]\n    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n    left, right = delta_w // 2, delta_w - (delta_w // 2)\n    padded = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n\n    # Step 4: Gaussian Noise Reduction\n    denoised = gaussian_filter(padded, sigma=1)\n\n    # Step 5: Normalize to [0,1]\n    normalized = denoised / 255.0\n\n    return normalized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T11:05:52.808920Z","iopub.execute_input":"2025-04-17T11:05:52.809546Z","iopub.status.idle":"2025-04-17T11:05:54.257477Z","shell.execute_reply.started":"2025-04-17T11:05:52.809497Z","shell.execute_reply":"2025-04-17T11:05:54.256872Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Step-2: Data Augmentation\nTo increase data diversity and prevent overfitting, the following augmentation techniques are applied:​\n\n* **Rotation:** Random rotations at various angles.​\n\n* **Flipping:** Horizontal and vertical flips.​\n\n* **Cropping:** Random crops to simulate zoom.​\n\n* **Brightness and Contrast Adjustment:** Randomly altering brightness and contrast levels.​\n\n* **Noise Addition:** Introducing random noise to images.","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\ndata_transforms = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T11:05:54.258301Z","iopub.execute_input":"2025-04-17T11:05:54.258651Z","iopub.status.idle":"2025-04-17T11:06:07.914840Z","shell.execute_reply.started":"2025-04-17T11:05:54.258630Z","shell.execute_reply":"2025-04-17T11:06:07.914186Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Step-3: Model Training with ResNet-18\nUtilizing transfer learning, a pre-trained ResNet-18 model is fine-tuned for skin lesion classification. The final fully connected layer is modified to output predictions for the nine classes present in the ISIC 2019 dataset.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# Load pre-trained ResNet-18 model\nmodel = models.resnet18(pretrained=True)\n\n# Freeze early layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Modify the final layer for 9-class classification\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 9)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T11:06:07.915565Z","iopub.execute_input":"2025-04-17T11:06:07.915957Z","iopub.status.idle":"2025-04-17T11:06:13.140532Z","shell.execute_reply.started":"2025-04-17T11:06:07.915937Z","shell.execute_reply":"2025-04-17T11:06:13.139907Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 156MB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Model Explainability with the LIME\nTo interpret the model's predictions, LIME (Local Interpretable Model-Agnostic Explanations) is employed. LIME provides visual explanations by highlighting regions in the image that most influenced the model's decision.","metadata":{}},{"cell_type":"code","source":"from lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nimport matplotlib.pyplot as plt\n\ndef explain_prediction(model, image_tensor):\n    model.eval()\n    image = image_tensor.numpy().transpose(1, 2, 0)\n\n    explainer = lime_image.LimeImageExplainer()\n    explanation = explainer.explain_instance(\n        image, \n        classifier_fn=lambda x: model(torch.tensor(x).permute(0, 3, 1, 2).float()).detach().numpy(),\n        top_labels=1,\n        hide_color=0,\n        num_samples=1000\n    )\n\n    temp, mask = explanation.get_image_and_mask(\n        explanation.top_labels[0],\n        positive_only=True,\n        num_features=5,\n        hide_rest=False\n    )\n\n    plt.imshow(mark_boundaries(temp / 255.0, mask))\n    plt.title('LIME Explanation')\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T11:06:21.289065Z","iopub.execute_input":"2025-04-17T11:06:21.289787Z","iopub.status.idle":"2025-04-17T11:06:21.296160Z","shell.execute_reply.started":"2025-04-17T11:06:21.289760Z","shell.execute_reply":"2025-04-17T11:06:21.295473Z"}},"outputs":[],"execution_count":5}]}