{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11557260,"sourceType":"datasetVersion","datasetId":7246604}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IEEE Access Paper: A Deep Learning Approach Based on Explainable Artificial Intelligence for Skin Lesion Classification\n**By: University of Engineering and Technology,Lahore,pakistan**","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom torch.amp import GradScaler, autocast\nimport cv2\nfrom skimage.feature import graycomatrix, graycoprops\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\nimport os\nimport multiprocessing as mp\nfrom functools import partial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:00:38.210200Z","iopub.execute_input":"2025-04-25T16:00:38.211141Z","iopub.status.idle":"2025-04-25T16:00:38.218157Z","shell.execute_reply.started":"2025-04-25T16:00:38.211108Z","shell.execute_reply":"2025-04-25T16:00:38.217344Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# GPU Information","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:01:04.006996Z","iopub.execute_input":"2025-04-25T16:01:04.007288Z","iopub.status.idle":"2025-04-25T16:01:04.245896Z","shell.execute_reply.started":"2025-04-25T16:01:04.007268Z","shell.execute_reply":"2025-04-25T16:01:04.245199Z"}},"outputs":[{"name":"stdout","text":"Fri Apr 25 16:01:04 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Step 1: Fix GPU detection\nprint(\"=== Starting GPU Setup ===\")\nprint(\"Uninstalling existing PyTorch packages...\")\n!pip uninstall -y torch torchvision torchaudio\nprint(\"Installing PyTorch with CUDA 11.8 support...\")\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Verify GPU availability\nprint(\"\\n=== Verifying GPU Availability ===\")\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Version:\", torch.version.cuda)\nprint(\"Number of GPUs:\", torch.cuda.device_count())\nif torch.cuda.is_available():\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"GPU not detected. Running on CPU. Please check PyTorch installation or Kaggle accelerator settings.\")\n    print(\"Falling back to CPU for now. We may switch to TensorFlow if this persists.\")\n\n# Define device early\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:01:09.191065Z","iopub.execute_input":"2025-04-25T16:01:09.191695Z","iopub.status.idle":"2025-04-25T16:04:10.373365Z","shell.execute_reply.started":"2025-04-25T16:01:09.191668Z","shell.execute_reply":"2025-04-25T16:04:10.372595Z"}},"outputs":[{"name":"stdout","text":"=== Starting GPU Setup ===\nUninstalling existing PyTorch packages...\nFound existing installation: torch 2.5.1+cu124\nUninstalling torch-2.5.1+cu124:\n  Successfully uninstalled torch-2.5.1+cu124\nFound existing installation: torchvision 0.20.1+cu124\nUninstalling torchvision-0.20.1+cu124:\n  Successfully uninstalled torchvision-0.20.1+cu124\nFound existing installation: torchaudio 2.5.1+cu124\nUninstalling torchaudio-2.5.1+cu124:\n  Successfully uninstalled torchaudio-2.5.1+cu124\nInstalling PyTorch with CUDA 11.8 support...\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.3.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n\n=== Verifying GPU Availability ===\nCUDA Available: True\nCUDA Version: 12.4\nNumber of GPUs: 2\nGPU Name: Tesla T4\nUsing device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Data Collection","metadata":{}},{"cell_type":"code","source":"# 2. Data Collection: Load DermaMNIST dataset\nprint(\"\\n=== Loading DermaMNIST Dataset ===\")\ndataset_path = \"/kaggle/input/dermamnist/dermamnist_224.npz\"\ndata = np.load(dataset_path)\ntrain_images = data['train_images']\ntrain_labels = data['train_labels'].flatten()\nval_images = data['val_images']\nval_labels = data['val_labels'].flatten()\ntest_images = data['test_images']\ntest_labels = data['test_labels'].flatten()\n\n# Class names for DermaMNIST\nclass_names = [\n    'basal cell carcinoma', 'squamous cell carcinoma', 'melanoma',\n    'actinic keratosis', 'benign keratosis', 'dermatofibroma', 'vascular lesion'\n]\nprint(\"Number of training images:\", len(train_images))\nprint(\"Number of validation images:\", len(val_images))\nprint(\"Number of test images:\", len(test_images))\nprint(\"Class distribution (train):\", np.bincount(train_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:04:27.062765Z","iopub.execute_input":"2025-04-25T16:04:27.063337Z","iopub.status.idle":"2025-04-25T16:04:46.453368Z","shell.execute_reply.started":"2025-04-25T16:04:27.063311Z","shell.execute_reply":"2025-04-25T16:04:46.452552Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading DermaMNIST Dataset ===\nNumber of training images: 7007\nNumber of validation images: 1003\nNumber of test images: 2005\nClass distribution (train): [ 228  359  769   80  779 4693   99]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Step-1: Image Preprocessing & Segmentation  \nTo enhance image quality and focus on the lesion areas, the following preprocessing steps are applied:​\n\n1.  **Objective:** Enhance image quality, standardize dimensions, and focus on regions of interest (ROIs).\n\n    I’ll preprocess images by:\n    \n    * Applying noise reduction (Gaussian blur).\n    * Resizing to 224x224 (standard for models like ResNet).\n    * Normalizing pixel values to [0, 1].\n    * Cropping ROIs (simplified to central cropping for now).\n1. **Implementation:**\n\nSince the dataset is large, we’ll preprocess images on-the-fly during training to save memory, using a generator.\nWe’ll assume metadata.csv has columns like isic_id (image filename without extension) and diagnosis (label).","metadata":{}},{"cell_type":"code","source":"# 3. Preprocessing: Precompute Segmentation and GLCM Features with Multiprocessing\nprint(\"\\n=== Preprocessing: Segmentation and GLCM Feature Extraction ===\")\n\ndef segment_image(image):\n    img = np.array(image)\n    mask = np.zeros(img.shape[:2], np.uint8)\n    bgdModel = np.zeros((1, 65), np.float64)\n    fgdModel = np.zeros((1, 65), np.float64)\n    height, width = img.shape[:2]\n    rect = (10, 10, width-20, height-20)\n    # Reduce iterations for faster processing (from 5 to 3)\n    cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 3, cv2.GC_INIT_WITH_RECT)\n    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    mask2 = cv2.dilate(mask2, kernel, iterations=1)\n    segmented = img * mask2[:, :, np.newaxis]\n    return segmented\n\ndef compute_glcm_features(image):\n    img = np.array(Image.fromarray(image).convert('L'))\n    glcm = graycomatrix(img, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n    features = {\n        'contrast': graycoprops(glcm, 'contrast')[0, 0],\n        'correlation': graycoprops(glcm, 'correlation')[0, 0],\n        'energy': graycoprops(glcm, 'energy')[0, 0],\n        'homogeneity': graycoprops(glcm, 'homogeneity')[0, 0]\n    }\n    return np.array([features['contrast'], features['correlation'], features['energy'], features['homogeneity']])\n\ndef process_image(img, idx, total):\n    if idx % 500 == 0:\n        print(f\"Processed {idx}/{total} images...\")\n    segmented = segment_image(img)\n    glcm = compute_glcm_features(segmented)\n    return segmented, glcm\n\n# Precompute and save segmented images and GLCM features using multiprocessing\noutput_dir = \"/kaggle/working/preprocessed_dermamnist\"\nos.makedirs(output_dir, exist_ok=True)\n\ndef preprocess_and_save(images, split_name):\n    segmented_path = os.path.join(output_dir, f\"{split_name}_segmented.npy\")\n    glcm_path = os.path.join(output_dir, f\"{split_name}_glcm.npy\")\n    \n    if os.path.exists(segmented_path) and os.path.exists(glcm_path):\n        print(f\"Loading precomputed {split_name} data...\")\n        segmented_images = np.load(segmented_path)\n        glcm_features = np.load(glcm_path)\n    else:\n        print(f\"Preprocessing {split_name} data ({len(images)} images) with multiprocessing...\")\n        num_cores = mp.cpu_count()\n        print(f\"Using {num_cores} CPU cores for preprocessing.\")\n        pool = mp.Pool(processes=num_cores)\n        process_func = partial(process_image, total=len(images))\n        results = pool.starmap(process_func, [(img, i) for i, img in enumerate(images)])\n        pool.close()\n        pool.join()\n        segmented_images = np.array([r[0] for r in results])\n        glcm_features = np.array([r[1] for r in results])\n        # Save to disk\n        np.save(segmented_path, segmented_images)\n        np.save(glcm_path, glcm_features)\n        print(f\"Saved precomputed {split_name} data.\")\n    \n    return segmented_images, glcm_features\n\n# Preprocess all splits\nprint(\"Preprocessing training set...\")\ntrain_segmented, train_glcm = preprocess_and_save(train_images, \"train\")\nprint(\"Preprocessing validation set...\")\nval_segmented, val_glcm = preprocess_and_save(val_images, \"val\")\nprint(\"Preprocessing test set...\")\ntest_segmented, test_glcm = preprocess_and_save(test_images, \"test\")\n\n# Custom Dataset for DermaMNIST\nclass DermaMNISTDataset(Dataset):\n    def __init__(self, images, glcm_features, labels, transform=None):\n        self.images = images\n        self.glcm_features = glcm_features\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = Image.fromarray(self.images[idx])\n        glcm_features = self.glcm_features[idx]\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, glcm_features, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:05:10.810203Z","iopub.execute_input":"2025-04-25T16:05:10.810732Z","iopub.status.idle":"2025-04-25T16:33:06.041332Z","shell.execute_reply.started":"2025-04-25T16:05:10.810708Z","shell.execute_reply":"2025-04-25T16:33:06.040589Z"}},"outputs":[{"name":"stdout","text":"\n=== Preprocessing: Segmentation and GLCM Feature Extraction ===\nPreprocessing training set...\nPreprocessing train data (7007 images) with multiprocessing...\nUsing 4 CPU cores for preprocessing.\nProcessed 0/7007 images...\nProcessed 500/7007 images...\nProcessed 1000/7007 images...\nProcessed 1500/7007 images...\nProcessed 2000/7007 images...\nProcessed 2500/7007 images...\nProcessed 3000/7007 images...\nProcessed 3500/7007 images...\nProcessed 4000/7007 images...\nProcessed 4500/7007 images...\nProcessed 5000/7007 images...\nProcessed 5500/7007 images...\nProcessed 6000/7007 images...\nProcessed 6500/7007 images...\nProcessed 7000/7007 images...\nSaved precomputed train data.\nPreprocessing validation set...\nPreprocessing val data (1003 images) with multiprocessing...\nUsing 4 CPU cores for preprocessing.\nProcessed 0/1003 images...\nProcessed 500/1003 images...\nProcessed 1000/1003 images...\nSaved precomputed val data.\nPreprocessing test set...\nPreprocessing test data (2005 images) with multiprocessing...\nUsing 4 CPU cores for preprocessing.\nProcessed 0/2005 images...\nProcessed 500/2005 images...\nProcessed 1000/2005 images...\nProcessed 1500/2005 images...\nProcessed 2000/2005 images...\nSaved precomputed test data.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Step-2: Data Augmentation\nTo increase data diversity and prevent overfitting, the following augmentation techniques are applied:​\n\n* **Rotation:** Random rotations at various angles.​\n\n* **Flipping:** Horizontal and vertical flips.​\n\n* **Cropping:** Random crops to simulate zoom.​\n\n* **Brightness and Contrast Adjustment:** Randomly altering brightness and contrast levels.​\n\n* **Noise Addition:** Introducing random noise to images.","metadata":{}},{"cell_type":"code","source":"# Transforms\ntrain_transforms = transforms.Compose([\n    transforms.RandomAffine(degrees=30, shear=10, scale=(0.8, 1.2)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1))\n])\nval_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:27:11.920514Z","iopub.execute_input":"2025-04-25T09:27:11.920854Z","iopub.status.idle":"2025-04-25T09:27:11.926138Z","shell.execute_reply.started":"2025-04-25T09:27:11.920831Z","shell.execute_reply":"2025-04-25T09:27:11.925370Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Step-3: Model Training with ResNet-18\nUtilizing transfer learning, a pre-trained ResNet-18 model is fine-tuned for skin lesion classification. The final fully connected layer is modified to output predictions for the nine classes present in the ISIC 2019 dataset.","metadata":{}},{"cell_type":"code","source":"# Create datasets\nprint(\"\\n=== Creating Datasets and DataLoaders ===\")\ntrain_dataset = DermaMNISTDataset(train_segmented, train_glcm, train_labels, transform=train_transforms)\nval_dataset = DermaMNISTDataset(val_segmented, val_glcm, val_labels, transform=val_transforms)\ntest_dataset = DermaMNISTDataset(test_segmented, test_glcm, test_labels, transform=val_transforms)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\nprint(f\"Created DataLoaders with batch size {batch_size}\")\n\n# 4. Classification: ResNet18 + MLP for Feature Fusion\nprint(\"\\n=== Setting Up Model ===\")\n\nclass CombinedModel(nn.Module):\n    def __init__(self, num_classes, glcm_feature_dim=4):\n        super(CombinedModel, self).__init__()\n        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        resnet_fc_dim = self.resnet.fc.in_features\n        self.resnet.fc = nn.Identity()\n        self.glcm_fc = nn.Linear(glcm_feature_dim, 64)\n        self.combined_fc = nn.Sequential(\n            nn.Linear(resnet_fc_dim + 64, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x, glcm_features):\n        resnet_features = self.resnet(x)\n        glcm_features = self.glcm_fc(glcm_features)\n        combined = torch.cat((resnet_features, glcm_features), dim=1)\n        output = self.combined_fc(combined)\n        return output\n\n# Normalize GLCM features\nprint(\"Normalizing GLCM features...\")\nscaler = StandardScaler()\nscaler.fit(train_glcm)\n\n# Compute class weights\nprint(\"Computing class weights...\")\nclass_counts = np.bincount(train_labels)\nclass_weights = torch.tensor([1.0 / count for count in class_counts], dtype=torch.float)\nclass_weights = class_weights / class_weights.sum() * len(class_counts)\nclass_weights = class_weights.to(device)\n\n# Initialize model\nmodel = CombinedModel(num_classes=len(class_names)).to(device)\nprint(f\"Model initialized on device: {next(model.parameters()).device}\")\n\n# Loss, optimizer, and scaler\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\nscaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Training loop\nprint(\"\\n=== Starting Training ===\")\nnum_epochs = 15\nbest_val_acc = 0.0\nfor epoch in range(num_epochs):\n    print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for i, data in enumerate(train_loader):\n        if i % 50 == 0:\n            print(f\"  Batch {i}/{len(train_loader)}\")\n        inputs, glcm_features, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        glcm_features = scaler.transform(glcm_features.numpy())\n        glcm_features = torch.tensor(glcm_features, dtype=torch.float32).to(device)\n        optimizer.zero_grad()\n        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n            outputs = model(inputs, glcm_features)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n\n    # Validation\n    print(f\"Validating Epoch {epoch+1}...\")\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, glcm_features, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            glcm_features = scaler.transform(glcm_features.numpy())\n            glcm_features = torch.tensor(glcm_features, dtype=torch.float32).to(device)\n            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n                outputs = model(inputs, glcm_features)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n    print(f\"Validation Acc: {val_acc:.2f}%\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pth\")\n        print(\"Saved best model with validation accuracy:\", best_val_acc)\n    if val_acc > 95:\n        print(\"Validation accuracy > 95%, stopping training.\")\n        break\n\n# Clear memory after training\ntorch.cuda.empty_cache()\nprint(\"Training complete, GPU memory cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:27:42.146412Z","iopub.execute_input":"2025-04-25T09:27:42.146939Z","iopub.status.idle":"2025-04-25T10:36:46.016084Z","shell.execute_reply.started":"2025-04-25T09:27:42.146911Z","shell.execute_reply":"2025-04-25T10:36:46.015170Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1264693862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_counts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"],"ename":"NameError","evalue":"name 'device' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# 4. Classification: ResNet18 + MLP for Feature Fusion\nclass CombinedModel(nn.Module):\n    def __init__(self, num_classes, glcm_feature_dim=4):\n        super(CombinedModel, self).__init__()\n        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        resnet_fc_dim = self.resnet.fc.in_features\n        self.resnet.fc = nn.Identity()\n        self.glcm_fc = nn.Linear(glcm_feature_dim, 64)\n        self.combined_fc = nn.Sequential(\n            nn.Linear(resnet_fc_dim + 64, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x, glcm_features):\n        resnet_features = self.resnet(x)\n        glcm_features = self.glcm_fc(glcm_features)\n        combined = torch.cat((resnet_features, glcm_features), dim=1)\n        output = self.combined_fc(combined)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:21:51.894531Z","iopub.execute_input":"2025-04-25T09:21:51.895214Z","iopub.status.idle":"2025-04-25T09:21:51.900199Z","shell.execute_reply.started":"2025-04-25T09:21:51.895190Z","shell.execute_reply":"2025-04-25T09:21:51.899466Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Normalize GLCM features\nscaler = StandardScaler()\nglcm_features_list = []\nfor idx in range(len(train_dataset)):\n    _, glcm_features, _ = train_dataset[idx]\n    glcm_features_list.append(glcm_features)\nglcm_features_list = np.array(glcm_features_list)\nscaler.fit(glcm_features_list)\n\n# Compute class weights\nclass_counts = np.bincount(train_labels)\nclass_weights = torch.tensor([1.0 / count for count in class_counts], dtype=torch.float)\nclass_weights = class_weights / class_weights.sum() * len(class_counts)\nclass_weights = class_weights.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:21:58.837094Z","iopub.execute_input":"2025-04-25T09:21:58.837577Z","iopub.status.idle":"2025-04-25T09:22:01.697920Z","shell.execute_reply.started":"2025-04-25T09:21:58.837536Z","shell.execute_reply":"2025-04-25T09:22:01.696948Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1752396085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mglcm_features_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglcm_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mglcm_features_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglcm_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mglcm_features_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglcm_features_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3342850184.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglcm_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1729\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m                 raise ValueError(\n\u001b[1;32m   1733\u001b[0m                     \u001b[0;34m\"If value is a sequence, it should have either a single value or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'Image' object has no attribute 'shape'","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CombinedModel(num_classes=len(class_names)).to(device)\nprint(f\"Training on device: {next(model.parameters()).device}\")\n\n# Loss, optimizer, and scaler\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\nscaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 15\nbest_val_acc = 0.0\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for data in train_loader:\n        inputs, glcm_features, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        glcm_features = scaler.transform(glcm_features.numpy())\n        glcm_features = torch.tensor(glcm_features, dtype=torch.float32).to(device)\n        optimizer.zero_grad()\n        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n            outputs = model(inputs, glcm_features)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, glcm_features, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            glcm_features = scaler.transform(glcm_features.numpy())\n            glcm_features = torch.tensor(glcm_features, dtype=torch.float32).to(device)\n            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n                outputs = model(inputs, glcm_features)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n    print(f\"Validation Acc: {val_acc:.2f}%\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pth\")\n    if val_acc > 95:\n        break\n\n# Clear memory after training\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Explainability with the LIME + GradCam\nTo interpret the model's predictions, LIME (Local Interpretable Model-Agnostic Explanations) is employed. LIME provides visual explanations by highlighting regions in the image that most influenced the model's decision.","metadata":{}},{"cell_type":"code","source":"# Grad-CAM implementation\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.hook_handles = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, input, output):\n            self.activations = output\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n        self.hook_handles.append(self.target_layer.register_backward_hook(backward_hook))\n\n    def __call__(self, x, glcm_features, index=None):\n        self.model.eval()\n        output = self.model(x, glcm_features)\n        if index is None:\n            index = torch.argmax(output, dim=1)\n        self.model.zero_grad()\n        score = output[0, index]\n        score.backward()\n        weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)\n        cam = torch.sum(weights * self.activations, dim=1)\n        cam = torch.relu(cam)\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-8)\n        return cam.cpu().detach().numpy()\n\n# Explainability\nmodel.eval()\nsample_idx = 0\nsample_image = Image.fromarray(val_images[sample_idx])\nsample_image = segment_image(sample_image)\nsample_glcm = compute_glcm_features(sample_image)\nsample_glcm = scaler.transform([sample_glcm])[0]\nsample_glcm = torch.tensor(sample_glcm, dtype=torch.float32).unsqueeze(0).to(device)\n\n# Preprocess for LIME and Grad-CAM\nlime_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nsample_tensor = lime_transform(sample_image).unsqueeze(0).to(device)\n\n# LIME\ndef predict_fn(images):\n    images = torch.from_numpy(images.transpose((0, 3, 1, 2))).float().to(device)\n    images = images / 255.0\n    images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(images)\n    glcm_batch = sample_glcm.repeat(images.shape[0], 1)\n    with torch.no_grad():\n        outputs = model(images, glcm_batch)\n    return outputs.cpu().numpy()\n\nexplainer = lime_image.LimeImageExplainer()\nexplanation = explainer.explain_instance(\n    np.array(sample_image),\n    predict_fn,\n    top_labels=5,\n    num_samples=500\n)\ntemp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5)\nplt.imshow(mark_boundaries(np.array(sample_image)/255.0, mask))\nplt.title(\"LIME Explanation\")\nplt.show()\n\n# Grad-CAM\ngradcam = GradCAM(model, model.resnet.layer4[-1])\ncam = gradcam(sample_tensor, sample_glcm)\ncam = cv2.resize(cam[0], (224, 224))\nplt.imshow(np.array(sample_image))\nplt.imshow(cam, cmap='jet', alpha=0.5)\nplt.title(\"Grad-CAM\")\nplt.show()\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:05:23.745735Z","iopub.status.idle":"2025-04-25T03:05:23.745951Z","shell.execute_reply.started":"2025-04-25T03:05:23.745843Z","shell.execute_reply":"2025-04-25T03:05:23.745852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Evaluate on test set\nmodel.eval()\nall_preds = []\nall_labels = []\nall_probs = []\nwith torch.no_grad():\n    for data in test_loader:\n        inputs, glcm_features, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        glcm_features = scaler.transform(glcm_features.numpy())\n        glcm_features = torch.tensor(glcm_features, dtype=torch.float32).to(device)\n        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n            outputs = model(inputs, glcm_features)\n        probs = torch.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n# Compute metrics\naccuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\n# Confusion Matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n# ROC-AUC\nroc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\nprint(f\"Test Accuracy: {accuracy:.2f}%\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1 Score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T03:05:23.747105Z","iopub.status.idle":"2025-04-25T03:05:23.747383Z","shell.execute_reply.started":"2025-04-25T03:05:23.747245Z","shell.execute_reply":"2025-04-25T03:05:23.747260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"from matplotlib.patches import FancyArrowPatch\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_xlim(0, 10)\nax.set_ylim(0, 6)\nax.axis('off')\n\nstages = [\n    (\"Data\\nCollection\", 1, 5),\n    (\"Segmentation\", 3, 5),\n    (\"Feature\\nExtraction\", 5, 5),\n    (\"Classification\", 7, 5),\n    (\"Evaluation\", 7, 3),\n    (\"XAI\", 5, 3)\n]\n\nfor text, x, y in stages:\n    ax.text(x, y, text, ha='center', va='center', bbox=dict(facecolor='lightblue', edgecolor='black'))\n\nfor i in range(len(stages)-1):\n    x1, y1 = stages[i][1], stages[i][2]\n    x2, y2 = stages[i+1][1], stages[i+1][2]\n    if y1 == y2:\n        arrow = FancyArrowPatch((x1+0.5, y1), (x2-0.5, y2), arrowstyle='->', mutation_scale=20, color='black')\n    else:\n        arrow = FancyArrowPatch((x1, y1-0.5), (x2, y2+0.5), arrowstyle='->', mutation_scale=20, color='black')\n    ax.add_patch(arrow)\n\nplt.title(\"Pipeline Flowchart for DermaMNIST\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}